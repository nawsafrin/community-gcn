Loading email dataset...
Epoch: 0001 loss_train: 3.7348 acc_train: 0.0700 loss_val: 3.7072 acc_val: 0.0202 time: 0.0326s
Epoch: 0002 loss_train: 3.7163 acc_train: 0.0700 loss_val: 3.6927 acc_val: 0.0202 time: 0.0353s
Epoch: 0003 loss_train: 3.6990 acc_train: 0.0700 loss_val: 3.6783 acc_val: 0.0202 time: 0.0344s
Epoch: 0004 loss_train: 3.6821 acc_train: 0.0700 loss_val: 3.6637 acc_val: 0.0202 time: 0.0344s
Epoch: 0005 loss_train: 3.6661 acc_train: 0.0700 loss_val: 3.6488 acc_val: 0.0202 time: 0.0364s
Epoch: 0006 loss_train: 3.6450 acc_train: 0.0700 loss_val: 3.6337 acc_val: 0.0202 time: 0.0353s
Epoch: 0007 loss_train: 3.6275 acc_train: 0.0700 loss_val: 3.6181 acc_val: 0.0202 time: 0.0345s
Epoch: 0008 loss_train: 3.6098 acc_train: 0.0700 loss_val: 3.6024 acc_val: 0.0202 time: 0.0345s
Epoch: 0009 loss_train: 3.5825 acc_train: 0.0700 loss_val: 3.5864 acc_val: 0.0202 time: 0.0344s
Epoch: 0010 loss_train: 3.5624 acc_train: 0.0700 loss_val: 3.5705 acc_val: 0.0202 time: 0.0343s
Epoch: 0011 loss_train: 3.5400 acc_train: 0.0700 loss_val: 3.5547 acc_val: 0.0202 time: 0.0342s
Epoch: 0012 loss_train: 3.5222 acc_train: 0.0700 loss_val: 3.5396 acc_val: 0.0202 time: 0.0343s
Epoch: 0013 loss_train: 3.4959 acc_train: 0.0700 loss_val: 3.5254 acc_val: 0.0202 time: 0.0342s
Epoch: 0014 loss_train: 3.4700 acc_train: 0.0700 loss_val: 3.5125 acc_val: 0.0202 time: 0.0371s
Epoch: 0015 loss_train: 3.4560 acc_train: 0.0700 loss_val: 3.5013 acc_val: 0.0202 time: 0.0343s
Epoch: 0016 loss_train: 3.4368 acc_train: 0.0700 loss_val: 3.4920 acc_val: 0.0202 time: 0.0344s
Epoch: 0017 loss_train: 3.4111 acc_train: 0.0700 loss_val: 3.4844 acc_val: 0.0202 time: 0.0352s
Epoch: 0018 loss_train: 3.4030 acc_train: 0.0700 loss_val: 3.4793 acc_val: 0.0202 time: 0.0351s
Epoch: 0019 loss_train: 3.3793 acc_train: 0.0700 loss_val: 3.4766 acc_val: 0.0202 time: 0.0344s
Epoch: 0020 loss_train: 3.3702 acc_train: 0.0700 loss_val: 3.4759 acc_val: 0.0202 time: 0.0376s
Epoch: 0021 loss_train: 3.3456 acc_train: 0.0700 loss_val: 3.4771 acc_val: 0.0202 time: 0.0350s
Epoch: 0022 loss_train: 3.3395 acc_train: 0.0700 loss_val: 3.4800 acc_val: 0.0202 time: 0.0360s
Epoch: 0023 loss_train: 3.3356 acc_train: 0.0700 loss_val: 3.4836 acc_val: 0.0202 time: 0.0359s
Epoch: 0024 loss_train: 3.3225 acc_train: 0.0700 loss_val: 3.4871 acc_val: 0.0202 time: 0.0346s
Epoch: 0025 loss_train: 3.3203 acc_train: 0.0700 loss_val: 3.4900 acc_val: 0.0202 time: 0.0378s
Epoch: 0026 loss_train: 3.3050 acc_train: 0.0700 loss_val: 3.4922 acc_val: 0.0202 time: 0.0351s
Epoch: 0027 loss_train: 3.3077 acc_train: 0.1300 loss_val: 3.4936 acc_val: 0.0000 time: 0.0345s
Epoch: 0028 loss_train: 3.3006 acc_train: 0.1000 loss_val: 3.4936 acc_val: 0.0000 time: 0.0349s
Epoch: 0029 loss_train: 3.2883 acc_train: 0.0900 loss_val: 3.4918 acc_val: 0.0000 time: 0.0346s
Epoch: 0030 loss_train: 3.2862 acc_train: 0.0900 loss_val: 3.4885 acc_val: 0.0000 time: 0.0345s
Epoch: 0031 loss_train: 3.2734 acc_train: 0.0900 loss_val: 3.4834 acc_val: 0.0000 time: 0.0368s
Epoch: 0032 loss_train: 3.2569 acc_train: 0.0900 loss_val: 3.4767 acc_val: 0.0000 time: 0.0347s
Epoch: 0033 loss_train: 3.2718 acc_train: 0.0900 loss_val: 3.4688 acc_val: 0.0000 time: 0.0346s
Epoch: 0034 loss_train: 3.2495 acc_train: 0.0900 loss_val: 3.4600 acc_val: 0.0000 time: 0.0356s
Epoch: 0035 loss_train: 3.2427 acc_train: 0.0900 loss_val: 3.4507 acc_val: 0.0000 time: 0.0344s
Epoch: 0036 loss_train: 3.2448 acc_train: 0.0900 loss_val: 3.4413 acc_val: 0.0000 time: 0.0345s
Epoch: 0037 loss_train: 3.2365 acc_train: 0.0900 loss_val: 3.4312 acc_val: 0.0000 time: 0.0344s
Epoch: 0038 loss_train: 3.2323 acc_train: 0.0900 loss_val: 3.4212 acc_val: 0.0000 time: 0.0342s
Epoch: 0039 loss_train: 3.2239 acc_train: 0.0900 loss_val: 3.4111 acc_val: 0.0000 time: 0.0341s
Epoch: 0040 loss_train: 3.2204 acc_train: 0.0900 loss_val: 3.4014 acc_val: 0.0000 time: 0.0342s
Epoch: 0041 loss_train: 3.2007 acc_train: 0.0900 loss_val: 3.3919 acc_val: 0.0000 time: 0.0345s
Epoch: 0042 loss_train: 3.2113 acc_train: 0.0900 loss_val: 3.3822 acc_val: 0.0000 time: 0.0362s
Epoch: 0043 loss_train: 3.1847 acc_train: 0.0900 loss_val: 3.3729 acc_val: 0.0000 time: 0.0353s
Epoch: 0044 loss_train: 3.1663 acc_train: 0.0900 loss_val: 3.3637 acc_val: 0.0000 time: 0.0345s
Epoch: 0045 loss_train: 3.1670 acc_train: 0.0900 loss_val: 3.3545 acc_val: 0.0000 time: 0.0346s
Epoch: 0046 loss_train: 3.1729 acc_train: 0.0900 loss_val: 3.3453 acc_val: 0.0000 time: 0.0347s
Epoch: 0047 loss_train: 3.1498 acc_train: 0.0900 loss_val: 3.3361 acc_val: 0.0000 time: 0.0346s
Epoch: 0048 loss_train: 3.1388 acc_train: 0.1000 loss_val: 3.3269 acc_val: 0.0000 time: 0.0355s
Epoch: 0049 loss_train: 3.1263 acc_train: 0.1150 loss_val: 3.3174 acc_val: 0.0000 time: 0.0356s
Epoch: 0050 loss_train: 3.1362 acc_train: 0.1000 loss_val: 3.3073 acc_val: 0.0000 time: 0.0346s
Epoch: 0051 loss_train: 3.1043 acc_train: 0.1400 loss_val: 3.2966 acc_val: 0.0101 time: 0.0346s
Epoch: 0052 loss_train: 3.0798 acc_train: 0.1600 loss_val: 3.2858 acc_val: 0.0202 time: 0.0346s
Epoch: 0053 loss_train: 3.0977 acc_train: 0.1500 loss_val: 3.2748 acc_val: 0.0202 time: 0.0346s
Epoch: 0054 loss_train: 3.0637 acc_train: 0.1750 loss_val: 3.2638 acc_val: 0.0202 time: 0.0346s
Epoch: 0055 loss_train: 3.0457 acc_train: 0.1850 loss_val: 3.2522 acc_val: 0.0202 time: 0.0347s
Epoch: 0056 loss_train: 3.0302 acc_train: 0.1900 loss_val: 3.2396 acc_val: 0.0202 time: 0.0346s
Epoch: 0057 loss_train: 3.0295 acc_train: 0.1850 loss_val: 3.2265 acc_val: 0.0202 time: 0.0345s
Epoch: 0058 loss_train: 3.0036 acc_train: 0.1950 loss_val: 3.2135 acc_val: 0.0202 time: 0.0346s
Epoch: 0059 loss_train: 2.9793 acc_train: 0.2000 loss_val: 3.2004 acc_val: 0.0202 time: 0.0345s
Epoch: 0060 loss_train: 2.9743 acc_train: 0.2000 loss_val: 3.1861 acc_val: 0.0202 time: 0.0344s
Epoch: 0061 loss_train: 2.9315 acc_train: 0.1950 loss_val: 3.1708 acc_val: 0.0202 time: 0.0346s
Epoch: 0062 loss_train: 2.9161 acc_train: 0.2050 loss_val: 3.1544 acc_val: 0.0303 time: 0.0347s
Epoch: 0063 loss_train: 2.8972 acc_train: 0.2150 loss_val: 3.1379 acc_val: 0.0404 time: 0.0348s
Epoch: 0064 loss_train: 2.8680 acc_train: 0.2150 loss_val: 3.1203 acc_val: 0.0505 time: 0.0350s
Epoch: 0065 loss_train: 2.8600 acc_train: 0.2350 loss_val: 3.1026 acc_val: 0.0707 time: 0.0401s
Epoch: 0066 loss_train: 2.8463 acc_train: 0.2550 loss_val: 3.0844 acc_val: 0.1010 time: 0.0351s
Epoch: 0067 loss_train: 2.8092 acc_train: 0.2600 loss_val: 3.0659 acc_val: 0.1313 time: 0.0351s
Epoch: 0068 loss_train: 2.7683 acc_train: 0.2650 loss_val: 3.0477 acc_val: 0.1515 time: 0.0346s
Epoch: 0069 loss_train: 2.7595 acc_train: 0.2550 loss_val: 3.0286 acc_val: 0.1818 time: 0.0351s
Epoch: 0070 loss_train: 2.7415 acc_train: 0.2600 loss_val: 3.0085 acc_val: 0.2222 time: 0.0350s
Epoch: 0071 loss_train: 2.7000 acc_train: 0.2750 loss_val: 2.9882 acc_val: 0.2626 time: 0.0356s
Epoch: 0072 loss_train: 2.7074 acc_train: 0.2900 loss_val: 2.9666 acc_val: 0.2828 time: 0.0355s
Epoch: 0073 loss_train: 2.6417 acc_train: 0.3400 loss_val: 2.9449 acc_val: 0.2929 time: 0.0352s
Epoch: 0074 loss_train: 2.6481 acc_train: 0.3650 loss_val: 2.9239 acc_val: 0.2929 time: 0.0347s
Epoch: 0075 loss_train: 2.6233 acc_train: 0.3550 loss_val: 2.9033 acc_val: 0.3030 time: 0.0351s
Epoch: 0076 loss_train: 2.6163 acc_train: 0.3400 loss_val: 2.8829 acc_val: 0.3333 time: 0.0346s
Epoch: 0077 loss_train: 2.5567 acc_train: 0.4100 loss_val: 2.8629 acc_val: 0.3434 time: 0.0373s
Epoch: 0078 loss_train: 2.5577 acc_train: 0.3950 loss_val: 2.8431 acc_val: 0.3434 time: 0.0349s
Epoch: 0079 loss_train: 2.5483 acc_train: 0.4150 loss_val: 2.8224 acc_val: 0.3434 time: 0.0352s
Epoch: 0080 loss_train: 2.5607 acc_train: 0.3600 loss_val: 2.8011 acc_val: 0.3434 time: 0.0356s
Epoch: 0081 loss_train: 2.4690 acc_train: 0.4600 loss_val: 2.7805 acc_val: 0.3535 time: 0.0353s
Epoch: 0082 loss_train: 2.4781 acc_train: 0.4300 loss_val: 2.7591 acc_val: 0.3535 time: 0.0353s
Epoch: 0083 loss_train: 2.4591 acc_train: 0.4200 loss_val: 2.7376 acc_val: 0.3535 time: 0.0372s
Epoch: 0084 loss_train: 2.4163 acc_train: 0.4400 loss_val: 2.7174 acc_val: 0.3535 time: 0.0368s
Epoch: 0085 loss_train: 2.4047 acc_train: 0.4450 loss_val: 2.6965 acc_val: 0.3636 time: 0.0359s
Epoch: 0086 loss_train: 2.3574 acc_train: 0.4650 loss_val: 2.6763 acc_val: 0.3737 time: 0.0344s
Epoch: 0087 loss_train: 2.3145 acc_train: 0.4450 loss_val: 2.6561 acc_val: 0.3737 time: 0.0346s
Epoch: 0088 loss_train: 2.3387 acc_train: 0.4450 loss_val: 2.6347 acc_val: 0.3737 time: 0.0345s
Epoch: 0089 loss_train: 2.3331 acc_train: 0.4550 loss_val: 2.6135 acc_val: 0.3939 time: 0.0345s
Epoch: 0090 loss_train: 2.2987 acc_train: 0.4900 loss_val: 2.5959 acc_val: 0.4040 time: 0.0345s
Epoch: 0091 loss_train: 2.2592 acc_train: 0.4850 loss_val: 2.5802 acc_val: 0.3939 time: 0.0345s
Epoch: 0092 loss_train: 2.2827 acc_train: 0.4600 loss_val: 2.5659 acc_val: 0.3939 time: 0.0345s
Epoch: 0093 loss_train: 2.2429 acc_train: 0.4550 loss_val: 2.5526 acc_val: 0.3939 time: 0.0344s
Epoch: 0094 loss_train: 2.2419 acc_train: 0.4650 loss_val: 2.5395 acc_val: 0.3939 time: 0.0352s
Epoch: 0095 loss_train: 2.1908 acc_train: 0.4800 loss_val: 2.5244 acc_val: 0.4141 time: 0.0346s
Epoch: 0096 loss_train: 2.2227 acc_train: 0.4900 loss_val: 2.5085 acc_val: 0.4242 time: 0.0345s
Epoch: 0097 loss_train: 2.1893 acc_train: 0.4750 loss_val: 2.4910 acc_val: 0.4242 time: 0.0345s
Epoch: 0098 loss_train: 2.1918 acc_train: 0.4900 loss_val: 2.4728 acc_val: 0.4343 time: 0.0345s
Epoch: 0099 loss_train: 2.1247 acc_train: 0.5150 loss_val: 2.4569 acc_val: 0.4545 time: 0.0345s
Epoch: 0100 loss_train: 2.1833 acc_train: 0.5300 loss_val: 2.4444 acc_val: 0.4444 time: 0.0350s
Epoch: 0101 loss_train: 2.1183 acc_train: 0.5050 loss_val: 2.4323 acc_val: 0.4444 time: 0.0346s
Epoch: 0102 loss_train: 2.1078 acc_train: 0.5450 loss_val: 2.4232 acc_val: 0.4343 time: 0.0346s
Epoch: 0103 loss_train: 2.1047 acc_train: 0.5100 loss_val: 2.4130 acc_val: 0.4242 time: 0.0347s
Epoch: 0104 loss_train: 2.0428 acc_train: 0.5450 loss_val: 2.4020 acc_val: 0.4141 time: 0.0346s
Epoch: 0105 loss_train: 2.0910 acc_train: 0.5200 loss_val: 2.3896 acc_val: 0.4242 time: 0.0353s
Epoch: 0106 loss_train: 2.0641 acc_train: 0.4900 loss_val: 2.3759 acc_val: 0.4242 time: 0.0361s
Epoch: 0107 loss_train: 2.1090 acc_train: 0.4800 loss_val: 2.3647 acc_val: 0.4343 time: 0.0373s
Epoch: 0108 loss_train: 2.0561 acc_train: 0.5100 loss_val: 2.3537 acc_val: 0.4343 time: 0.0348s
Epoch: 0109 loss_train: 2.0563 acc_train: 0.5150 loss_val: 2.3461 acc_val: 0.4343 time: 0.0346s
Epoch: 0110 loss_train: 2.0438 acc_train: 0.5400 loss_val: 2.3359 acc_val: 0.4545 time: 0.0345s
Epoch: 0111 loss_train: 2.0754 acc_train: 0.4650 loss_val: 2.3266 acc_val: 0.4747 time: 0.0345s
Epoch: 0112 loss_train: 2.0405 acc_train: 0.5250 loss_val: 2.3166 acc_val: 0.4646 time: 0.0345s
Epoch: 0113 loss_train: 2.0226 acc_train: 0.5500 loss_val: 2.3058 acc_val: 0.4747 time: 0.0348s
Epoch: 0114 loss_train: 2.0054 acc_train: 0.5600 loss_val: 2.2947 acc_val: 0.4848 time: 0.0348s
Epoch: 0115 loss_train: 2.0034 acc_train: 0.5500 loss_val: 2.2850 acc_val: 0.4747 time: 0.0346s
Epoch: 0116 loss_train: 2.0158 acc_train: 0.5050 loss_val: 2.2770 acc_val: 0.4747 time: 0.0345s
Epoch: 0117 loss_train: 1.9454 acc_train: 0.5750 loss_val: 2.2688 acc_val: 0.4747 time: 0.0355s
Epoch: 0118 loss_train: 1.9566 acc_train: 0.5450 loss_val: 2.2622 acc_val: 0.4747 time: 0.0346s
Epoch: 0119 loss_train: 1.9212 acc_train: 0.5150 loss_val: 2.2531 acc_val: 0.4747 time: 0.0346s
Epoch: 0120 loss_train: 1.9745 acc_train: 0.5450 loss_val: 2.2450 acc_val: 0.4848 time: 0.0346s
Epoch: 0121 loss_train: 1.9748 acc_train: 0.5450 loss_val: 2.2365 acc_val: 0.4949 time: 0.0347s
Epoch: 0122 loss_train: 1.9002 acc_train: 0.5350 loss_val: 2.2267 acc_val: 0.4949 time: 0.0346s
Epoch: 0123 loss_train: 1.9469 acc_train: 0.5650 loss_val: 2.2168 acc_val: 0.5152 time: 0.0347s
Epoch: 0124 loss_train: 1.9084 acc_train: 0.5750 loss_val: 2.2060 acc_val: 0.5152 time: 0.0345s
Epoch: 0125 loss_train: 1.9025 acc_train: 0.5450 loss_val: 2.1937 acc_val: 0.5152 time: 0.0344s
Epoch: 0126 loss_train: 1.9082 acc_train: 0.5850 loss_val: 2.1806 acc_val: 0.5152 time: 0.0345s
Epoch: 0127 loss_train: 1.8927 acc_train: 0.5600 loss_val: 2.1717 acc_val: 0.5152 time: 0.0343s
Epoch: 0128 loss_train: 1.8550 acc_train: 0.5600 loss_val: 2.1630 acc_val: 0.5253 time: 0.0342s
Epoch: 0129 loss_train: 1.8922 acc_train: 0.5650 loss_val: 2.1546 acc_val: 0.5253 time: 0.0345s
Epoch: 0130 loss_train: 1.9034 acc_train: 0.5800 loss_val: 2.1492 acc_val: 0.5051 time: 0.0347s
Epoch: 0131 loss_train: 1.8615 acc_train: 0.5850 loss_val: 2.1437 acc_val: 0.5051 time: 0.0344s
Epoch: 0132 loss_train: 1.8746 acc_train: 0.5600 loss_val: 2.1369 acc_val: 0.4848 time: 0.0345s
Epoch: 0133 loss_train: 1.7959 acc_train: 0.5900 loss_val: 2.1308 acc_val: 0.4949 time: 0.0346s
Epoch: 0134 loss_train: 1.8657 acc_train: 0.5400 loss_val: 2.1241 acc_val: 0.5051 time: 0.0392s
Epoch: 0135 loss_train: 1.8606 acc_train: 0.5450 loss_val: 2.1124 acc_val: 0.5152 time: 0.0355s
Epoch: 0136 loss_train: 1.8067 acc_train: 0.5500 loss_val: 2.1000 acc_val: 0.5253 time: 0.0345s
Epoch: 0137 loss_train: 1.8218 acc_train: 0.5700 loss_val: 2.0864 acc_val: 0.5455 time: 0.0345s
Epoch: 0138 loss_train: 1.8157 acc_train: 0.5850 loss_val: 2.0750 acc_val: 0.5455 time: 0.0343s
Epoch: 0139 loss_train: 1.8540 acc_train: 0.5500 loss_val: 2.0679 acc_val: 0.5556 time: 0.0341s
Epoch: 0140 loss_train: 1.8051 acc_train: 0.5900 loss_val: 2.0628 acc_val: 0.5657 time: 0.0342s
Epoch: 0141 loss_train: 1.8102 acc_train: 0.5700 loss_val: 2.0569 acc_val: 0.5657 time: 0.0343s
Epoch: 0142 loss_train: 1.8056 acc_train: 0.5750 loss_val: 2.0529 acc_val: 0.5556 time: 0.0343s
Epoch: 0143 loss_train: 1.7807 acc_train: 0.5750 loss_val: 2.0508 acc_val: 0.5556 time: 0.0343s
Epoch: 0144 loss_train: 1.7935 acc_train: 0.6000 loss_val: 2.0495 acc_val: 0.5455 time: 0.0345s
Epoch: 0145 loss_train: 1.7290 acc_train: 0.5850 loss_val: 2.0481 acc_val: 0.5556 time: 0.0343s
Epoch: 0146 loss_train: 1.7348 acc_train: 0.6250 loss_val: 2.0480 acc_val: 0.5455 time: 0.0343s
Epoch: 0147 loss_train: 1.7667 acc_train: 0.5750 loss_val: 2.0438 acc_val: 0.5455 time: 0.0346s
Epoch: 0148 loss_train: 1.7646 acc_train: 0.5650 loss_val: 2.0369 acc_val: 0.5556 time: 0.0344s
Epoch: 0149 loss_train: 1.6796 acc_train: 0.5900 loss_val: 2.0287 acc_val: 0.5354 time: 0.0344s
Epoch: 0150 loss_train: 1.7261 acc_train: 0.5900 loss_val: 2.0209 acc_val: 0.5354 time: 0.0346s
Epoch: 0151 loss_train: 1.7381 acc_train: 0.5800 loss_val: 2.0100 acc_val: 0.5354 time: 0.0358s
Epoch: 0152 loss_train: 1.7380 acc_train: 0.6300 loss_val: 2.0009 acc_val: 0.5354 time: 0.0352s
Epoch: 0153 loss_train: 1.7509 acc_train: 0.5900 loss_val: 1.9941 acc_val: 0.5354 time: 0.0343s
Epoch: 0154 loss_train: 1.7533 acc_train: 0.5900 loss_val: 1.9872 acc_val: 0.5556 time: 0.0379s
Epoch: 0155 loss_train: 1.6924 acc_train: 0.6100 loss_val: 1.9813 acc_val: 0.5556 time: 0.0345s
Epoch: 0156 loss_train: 1.7256 acc_train: 0.6150 loss_val: 1.9751 acc_val: 0.5556 time: 0.0343s
Epoch: 0157 loss_train: 1.7266 acc_train: 0.5750 loss_val: 1.9684 acc_val: 0.5657 time: 0.0342s
Epoch: 0158 loss_train: 1.6993 acc_train: 0.6050 loss_val: 1.9608 acc_val: 0.5758 time: 0.0346s
Epoch: 0159 loss_train: 1.7134 acc_train: 0.5900 loss_val: 1.9532 acc_val: 0.5758 time: 0.0344s
Epoch: 0160 loss_train: 1.7376 acc_train: 0.5900 loss_val: 1.9501 acc_val: 0.5657 time: 0.0343s
Epoch: 0161 loss_train: 1.7348 acc_train: 0.5900 loss_val: 1.9468 acc_val: 0.5657 time: 0.0343s
Epoch: 0162 loss_train: 1.7580 acc_train: 0.5750 loss_val: 1.9426 acc_val: 0.5657 time: 0.0342s
Epoch: 0163 loss_train: 1.6848 acc_train: 0.5900 loss_val: 1.9399 acc_val: 0.5758 time: 0.0355s
Epoch: 0164 loss_train: 1.6869 acc_train: 0.5850 loss_val: 1.9369 acc_val: 0.5758 time: 0.0344s
Epoch: 0165 loss_train: 1.7344 acc_train: 0.6150 loss_val: 1.9348 acc_val: 0.5758 time: 0.0344s
Epoch: 0166 loss_train: 1.6589 acc_train: 0.5950 loss_val: 1.9355 acc_val: 0.5657 time: 0.0342s
Epoch: 0167 loss_train: 1.6593 acc_train: 0.6250 loss_val: 1.9357 acc_val: 0.5657 time: 0.0342s
Epoch: 0168 loss_train: 1.6723 acc_train: 0.5950 loss_val: 1.9334 acc_val: 0.5657 time: 0.0345s
Epoch: 0169 loss_train: 1.6952 acc_train: 0.5950 loss_val: 1.9281 acc_val: 0.5556 time: 0.0362s
Epoch: 0170 loss_train: 1.6889 acc_train: 0.5850 loss_val: 1.9226 acc_val: 0.5556 time: 0.0342s
Epoch: 0171 loss_train: 1.6727 acc_train: 0.6000 loss_val: 1.9128 acc_val: 0.5556 time: 0.0343s
Epoch: 0172 loss_train: 1.6613 acc_train: 0.5900 loss_val: 1.9021 acc_val: 0.5556 time: 0.0348s
Epoch: 0173 loss_train: 1.6269 acc_train: 0.6000 loss_val: 1.8940 acc_val: 0.5556 time: 0.0345s
Epoch: 0174 loss_train: 1.6459 acc_train: 0.5950 loss_val: 1.8889 acc_val: 0.5657 time: 0.0345s
Epoch: 0175 loss_train: 1.6518 acc_train: 0.5900 loss_val: 1.8856 acc_val: 0.5657 time: 0.0346s
Epoch: 0176 loss_train: 1.6403 acc_train: 0.6050 loss_val: 1.8817 acc_val: 0.5657 time: 0.0346s
Epoch: 0177 loss_train: 1.6261 acc_train: 0.6050 loss_val: 1.8773 acc_val: 0.5758 time: 0.0344s
Epoch: 0178 loss_train: 1.6161 acc_train: 0.6150 loss_val: 1.8714 acc_val: 0.5758 time: 0.0347s
Epoch: 0179 loss_train: 1.6038 acc_train: 0.6000 loss_val: 1.8647 acc_val: 0.5758 time: 0.0348s
Epoch: 0180 loss_train: 1.6131 acc_train: 0.6150 loss_val: 1.8566 acc_val: 0.5758 time: 0.0348s
Epoch: 0181 loss_train: 1.6397 acc_train: 0.6200 loss_val: 1.8487 acc_val: 0.5758 time: 0.0348s
Epoch: 0182 loss_train: 1.6522 acc_train: 0.6200 loss_val: 1.8426 acc_val: 0.5758 time: 0.0347s
Epoch: 0183 loss_train: 1.6257 acc_train: 0.6350 loss_val: 1.8403 acc_val: 0.5556 time: 0.0347s
Epoch: 0184 loss_train: 1.6075 acc_train: 0.5950 loss_val: 1.8364 acc_val: 0.5455 time: 0.0354s
Epoch: 0185 loss_train: 1.5731 acc_train: 0.5950 loss_val: 1.8338 acc_val: 0.5455 time: 0.0346s
Epoch: 0186 loss_train: 1.6231 acc_train: 0.5950 loss_val: 1.8332 acc_val: 0.5455 time: 0.0385s
Epoch: 0187 loss_train: 1.6082 acc_train: 0.6200 loss_val: 1.8310 acc_val: 0.5657 time: 0.0347s
Epoch: 0188 loss_train: 1.5606 acc_train: 0.6000 loss_val: 1.8304 acc_val: 0.5657 time: 0.0343s
Epoch: 0189 loss_train: 1.5876 acc_train: 0.6250 loss_val: 1.8279 acc_val: 0.5657 time: 0.0343s
Epoch: 0190 loss_train: 1.5948 acc_train: 0.6000 loss_val: 1.8297 acc_val: 0.5657 time: 0.0344s
Epoch: 0191 loss_train: 1.6241 acc_train: 0.6100 loss_val: 1.8289 acc_val: 0.5859 time: 0.0345s
Epoch: 0192 loss_train: 1.5352 acc_train: 0.6350 loss_val: 1.8290 acc_val: 0.5859 time: 0.0378s
Epoch: 0193 loss_train: 1.6258 acc_train: 0.5950 loss_val: 1.8225 acc_val: 0.5859 time: 0.0348s
Epoch: 0194 loss_train: 1.5448 acc_train: 0.6200 loss_val: 1.8128 acc_val: 0.5758 time: 0.0350s
Epoch: 0195 loss_train: 1.5894 acc_train: 0.6250 loss_val: 1.8033 acc_val: 0.5657 time: 0.0348s
Epoch: 0196 loss_train: 1.5413 acc_train: 0.6300 loss_val: 1.7981 acc_val: 0.5657 time: 0.0348s
Epoch: 0197 loss_train: 1.5575 acc_train: 0.6000 loss_val: 1.7940 acc_val: 0.5758 time: 0.0348s
Epoch: 0198 loss_train: 1.5562 acc_train: 0.6100 loss_val: 1.7894 acc_val: 0.5758 time: 0.0352s
Epoch: 0199 loss_train: 1.5790 acc_train: 0.6100 loss_val: 1.7885 acc_val: 0.5758 time: 0.0346s
Epoch: 0200 loss_train: 1.5247 acc_train: 0.6350 loss_val: 1.7898 acc_val: 0.5758 time: 0.0348s
Optimization Finished!
Total time elapsed: 6.9903s
Confusion Matrix:

Predicted  0   1   2  3  4  5  6  7  8  ...  34  35  36  37  38  39  40  41  __all__
Actual                                  ...                                         
0          0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        1
1          0  19   0  0  0  0  0  0  0  ...   0   0   0   1   6   0   0   0       42
2          0   0  20  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       24
3          0   0   0  4  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        7
4          0   0   0  0  0  0  0  0  0  ...   0   0   0   0   1   0   0   0       11
5          0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        1
6          0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        3
7          0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        7
8          0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       26
9          0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       10
10         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       11
11         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        7
12         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        9
13         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        3
14         0   2   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        7
15         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       11
16         0   2   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        2
17         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       19
18         0   0   0  1  0  0  0  0  0  ...   0   0   0   1   0   0   0   0       41
19         0   7   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       24
20         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       17
21         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        8
22         0   0   0  1  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       10
23         0   0   0  1  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       24
24         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       73
25         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        1
26         0   4   0  0  0  0  0  0  0  ...   0   0   0   1   3   0   0   0       25
27         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       10
28         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        4
29         0   1   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       17
30         0   1   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       45
31         0   1   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        2
32         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        5
33         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   2   0   0   0       78
34         0   0   1  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       13
35         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0       10
36         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        8
37         0   0   0  0  0  0  0  0  0  ...   0   0   0  36   1   0   0   0       40
38         0   0   0  0  0  0  0  0  0  ...   0   0   0   2  24   0   0   0       31
39         0   0   0  1  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        5
40         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        4
41         0   0   0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0        9
__all__    0  37  21  8  0  0  0  0  0  ...   0   0   0  41  37   0   0   0      705

[43 rows x 43 columns]


Overall Statistics:

Accuracy: 0.5163120567375886
95% CI: (0.478711839997356, 0.5537752395463944)
No Information Rate: ToDo
P-Value [Acc > NIR]: 1.0665856207364903e-84
Kappa: 0.4852251557782488
Mcnemar's Test P-Value: ToDo


Class Statistics:

Classes                                        0   ...        41
Population                                    705  ...       705
P: Condition positive                           1  ...         9
N: Condition negative                         704  ...       696
Test outcome positive                           0  ...         0
Test outcome negative                         705  ...       705
TP: True Positive                               0  ...         0
TN: True Negative                             704  ...       696
FP: False Positive                              0  ...         0
FN: False Negative                              1  ...         9
TPR: (Sensitivity, hit rate, recall)            0  ...         0
TNR=SPC: (Specificity)                          1  ...         1
PPV: Pos Pred Value (Precision)               NaN  ...       NaN
NPV: Neg Pred Value                      0.998582  ...  0.987234
FPR: False-out                                  0  ...         0
FDR: False Discovery Rate                     NaN  ...       NaN
FNR: Miss Rate                                  1  ...         1
ACC: Accuracy                            0.998582  ...  0.987234
F1 score                                        0  ...         0
MCC: Matthews correlation coefficient         NaN  ...       NaN
Informedness                                    0  ...         0
Markedness                                    NaN  ...       NaN
Prevalence                             0.00141844  ...  0.012766
LR+: Positive likelihood ratio                NaN  ...       NaN
LR-: Negative likelihood ratio                  1  ...         1
DOR: Diagnostic odds ratio                    NaN  ...       NaN
FOR: False omission rate               0.00141844  ...  0.012766

[26 rows x 42 columns]
0.0
0.4523809523809524
0.8333333333333334
0.5714285714285714
0.0
0.0
0.0
0.0
0.0
0.0
0.5454545454545454
0.0
0.0
0.0
0.0
1.0
0.0
0.0
0.8536585365853658
0.0
0.8235294117647058
0.0
0.8
0.4166666666666667
0.958904109589041
0.0
0.0
0.0
0.0
0.0
0.8222222222222222
0.0
0.0
0.8974358974358975
0.0
0.0
0.0
0.9
0.7741935483870968
0.0
0.0
0.0
0.5163120567375886
0.0
0.4523809523809524
0.8333333333333334
0.5714285714285714
0.0
0.0
0.0
0.0
0.0
0.0
0.5454545454545454
0.0
0.0
0.0
0.0
1.0
0.0
0.0
0.8536585365853658
0.0
0.8235294117647058
0.0
0.8
0.4166666666666667
0.958904109589041
0.0
0.0
0.0
0.0
0.0
0.8222222222222222
0.0
0.0
0.8974358974358975
0.0
0.0
0.0
0.9
0.7741935483870968
0.0
0.0
0.0
Recall/Sensitivity: 0.52
1.0
0.9728506787330317
0.9985315712187959
0.994269340974212
1.0
1.0
1.0
1.0
1.0
1.0
0.9927953890489913
1.0
1.0
1.0
1.0
0.920749279538905
1.0
1.0
0.9427710843373494
1.0
0.997093023255814
1.0
0.8920863309352518
0.9955947136563876
0.9968354430379747
1.0
1.0
1.0
1.0
1.0
0.9136363636363637
1.0
1.0
0.8995215311004785
1.0
1.0
1.0
0.9924812030075187
0.9807121661721068
1.0
1.0
1.0
Specificity: 0.97
nan
0.45135460570897384
0.8873486254035008
0.5295414067711489
nan
nan
nan
nan
nan
nan
0.5382499345035369
nan
nan
nan
nan
0.391737428970585
nan
nan
0.6117914493891315
nan
0.8452641768131892
nan
0.2539325860488159
0.5556829946590205
0.9615932926041265
nan
nan
nan
nan
nan
0.5291612750491621
nan
nan
0.6389874963058921
nan
nan
nan
0.8821936517113682
0.694087806390769
nan
nan
nan
MCC: 0.44
Test set results: loss= 2.0777 accuracy= 0.5163
